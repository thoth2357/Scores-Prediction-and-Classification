{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from EDA import Preprocessing\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from IPython.display import display\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read datasets\n",
    "import pandas as pd\n",
    "\n",
    "df_mat = pd.read_csv('/home/pirate/Documents/machine_learning/score_prediction/student/student-mat.csv',sep=';')\n",
    "df_por = pd.read_csv('/home/pirate/Documents/machine_learning/score_prediction/student/student-por.csv',sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling duplicates found in datasets\n",
    "df_merged=df_mat.append(df_por, ignore_index=True)\n",
    "no_of_duplicates = df_merged.duplicated([\"school\",\"sex\",\"age\",\"address\",\"famsize\",\"Pstatus\",\"Medu\",\"Fedu\",\"Mjob\",\"Fjob\",\"reason\",\"nursery\",\"internet\"]).sum()\n",
    "print(f'---> We have {no_of_duplicates} found in our two datasets\\n')\n",
    "\n",
    "print('---> Merging datasets and removing duplicates\\n')\n",
    "df_merged.drop_duplicates(subset=[\"school\",\"sex\",\"age\",\"address\",\"famsize\",\"Pstatus\",\"Medu\",\"Fedu\",\"Mjob\",\"Fjob\",\"reason\",\"nursery\",\"internet\"],\n",
    "                 keep = 'first', inplace = True)\n",
    "\n",
    "print(f'---> After merging and removing duplicates we have {len(df_merged)} unique rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    # PERFORMING EDA\n",
    "preprocessor = Preprocessing(df_merged)\n",
    "#check for missing values\n",
    "df = preprocessor.check_missing_value()\n",
    "\n",
    "#check dataset descriptives\n",
    "preprocessor.descriptives(df)\n",
    "\n",
    "#perform data exploration\n",
    "##separating dataset into categorical and continuous columns\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "continous_columns = numerical_columns_selector(df)\n",
    "categorical_columns = categorical_columns_selector(df)\n",
    "\n",
    "feature_list = list(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette('pastel')[0:5]\n",
    "\n",
    "#performing counts of catgorical variable levels for some variables\n",
    "# sex_count_table = pd.crosstab(index=df['sex'], columns='freq')\n",
    "sns.countplot(x ='sex', data = df)\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(x ='guardian', y ='studytime', data = df, hue ='sex')\n",
    "plt.show()\n",
    "\n",
    "sns.stripplot(x ='school', y ='G3', data = df, jitter = True, hue ='sex', dodge = True)\n",
    "plt.show()\n",
    "\n",
    "# sns.pairplot(df, hue =\"sex\", palette ='coolwarm')\n",
    "# plt.show()\n",
    "\n",
    "g = sns.catplot(x='guardian',y='studytime', col = 'romantic', data=df,\n",
    "                kind='bar', aspect=.6, palette='Set2')\n",
    "(g.set_axis_labels(\"Guardian\", \"Study Time\")\n",
    "  .set_titles(\"relationship : {col_name}\"))\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(x='internet',y='G3',data=df, palette='rainbow', hue='sex')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET ENCODING OF CATEGORICAL VARIABLES\n",
    "df_copy = df.copy() #copying the dataset to preserve original copy\n",
    "ord_enc = OrdinalEncoder() #encoder object\n",
    "\n",
    "#looping through all the categorical columns initailly found and applying encoding on it\n",
    "print('Performing EnCoding Of Categorical variables')\n",
    "for col in categorical_columns:\n",
    "    df_copy[col] = ord_enc.fit_transform(df_copy[[col]])\n",
    "df_copy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        #REGRESSION\n",
    "#splitting datasets into dependent and independent variables\n",
    "X = df_copy.drop('G3',axis=1)\n",
    "Y = df_copy['G3']\n",
    "\n",
    "#splitting X, Y into train and test (80:20) ratio\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "# Convert to numpy array\n",
    "X = np.array(X)\n",
    "\n",
    "#show summary of splitted datasets\n",
    "print(\"shape of original dataset :\", df_copy.shape)\n",
    "print(\"shape of input - training set\", x_train.shape)\n",
    "print(\"shape of output - training set\", y_train.shape)\n",
    "print(\"shape of input - testing set\", x_test.shape)\n",
    "print(\"shape of output - testing set\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                            # Random Forest Regression\n",
    "regressor = RandomForestRegressor(n_estimators = 20, random_state = 0)\n",
    "\n",
    "# fit the regressor with x and y data\n",
    "regressor.fit(x_train, y_train)\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "y_pred = regressor.predict(x_test)\n",
    "# Metrics\n",
    "errors = abs(y_pred - y_test)\n",
    "mape = 100 * np.mean(errors / y_test)\n",
    "accuracy = 100 - mape\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# # Get numerical feature importances\n",
    "importances = list(regressor.feature_importances_)\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# # Print out the feature and importances and plot them\n",
    "x, y = zip(*feature_importances)\n",
    "plt.xticks(rotation=90)\n",
    "plt.bar(x, y)\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                            #CLASSIFICATION\n",
    "#DIVIDE GRADE G3 INTO CLASSES \n",
    "\"\"\"\n",
    "'Low' :- greater than 0 upto 6\n",
    "'Medium' :- greater than 6 upto 14\n",
    "'High' :- greater than 14 upto 20\n",
    "\"\"\"\n",
    "df_copy['G3_Class'] = pd.cut(x=df_copy['G3'], bins=[0,6,14,20], labels=['Low', 'Medium', 'High'])\n",
    "# df_copy.head(20)\n",
    "\n",
    "#plotting for class imbalance\n",
    "sns.countplot('G3_Class', data=df_copy)\n",
    "\n",
    "#class count\n",
    "count_class_medium, count_class_high, count_class_low = df_copy['G3_Class'].value_counts()\n",
    "\n",
    "# Divide by class\n",
    "class_low = df_copy[df_copy['G3_Class'] == 'Low']\n",
    "class_medium = df_copy[df_copy['G3_Class'] == 'Medium']\n",
    "class_high = df_copy[df_copy['G3_Class'] == 'High']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solving Class imbalance problem\n",
    "                        ###using sampling method###\n",
    "\n",
    "# Random Over-Sampling\n",
    "class_low_over = class_low.sample(count_class_medium, replace=True)\n",
    "df_low_over = pd.concat([class_medium, class_low_over], axis=0)\n",
    "combined_dataset = df_low_over.append(class_high, ignore_index=True)\n",
    "print('Performing Over sampling technique to increase low class to medium class size while maintaining high class value')\n",
    "print(combined_dataset.G3_Class.value_counts(), '\\n')\n",
    "\n",
    "#class count\n",
    "count_class_medium, count_class_low, count_class_high = combined_dataset['G3_Class'].value_counts()\n",
    "# # Divide by class\n",
    "class_low = combined_dataset[combined_dataset['G3_Class'] == 'Low']\n",
    "class_medium = combined_dataset[combined_dataset['G3_Class'] == 'Medium']\n",
    "class_high = combined_dataset[combined_dataset['G3_Class'] == 'High']\n",
    "\n",
    "class_high_over = class_high.sample(count_class_medium, replace=True)\n",
    "df_high_over = pd.concat([class_medium, class_high_over], axis=0)\n",
    "dataset_final_sampling1 = df_high_over.append(class_low, ignore_index=True)\n",
    "print('Performing Over sampling technique to increase low class to medium class size while maintaining high class value')\n",
    "print(dataset_final_sampling1 .G3_Class.value_counts(),\"\\n\")\n",
    "\n",
    "#plotting dataset that has been sampled\n",
    "dataset_final_sampling1.G3_Class.value_counts().plot(kind='bar', title='Count (target)');\n",
    "\n",
    "x_train_new,x_test_new,y_train_new,y_test_new=train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing EnCoding Of Categorical variables\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "      <th>G3_Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      school  sex  age  address  famsize  Pstatus  Medu  Fedu  Mjob  Fjob  \\\n",
       "0        0.0  0.0   15      1.0      1.0      1.0     1     1   0.0   2.0   \n",
       "1        0.0  0.0   16      1.0      0.0      1.0     3     3   2.0   2.0   \n",
       "2        0.0  1.0   16      1.0      1.0      1.0     2     2   2.0   2.0   \n",
       "3        0.0  0.0   15      1.0      0.0      1.0     4     4   4.0   1.0   \n",
       "4        0.0  0.0   15      1.0      0.0      1.0     2     1   3.0   2.0   \n",
       "...      ...  ...  ...      ...      ...      ...   ...   ...   ...   ...   \n",
       "1435     0.0  0.0   17      1.0      0.0      1.0     1     1   0.0   2.0   \n",
       "1436     0.0  0.0   16      1.0      0.0      1.0     3     1   3.0   2.0   \n",
       "1437     1.0  0.0   17      0.0      0.0      1.0     1     2   2.0   2.0   \n",
       "1438     0.0  1.0   17      1.0      0.0      1.0     3     2   3.0   3.0   \n",
       "1439     0.0  0.0   17      1.0      0.0      1.0     4     3   2.0   2.0   \n",
       "\n",
       "      ...  freetime  goout  Dalc  Walc  health  absences  G1  G2  G3  G3_Class  \n",
       "0     ...         3      2     2     3       3        10   7   8  10       2.0  \n",
       "1     ...         3      2     1     2       5         4   6  10  10       2.0  \n",
       "2     ...         4      4     1     1       3         0  12  12  11       2.0  \n",
       "3     ...         3      3     1     2       2         0  10   8   9       2.0  \n",
       "4     ...         2      2     1     1       4         4  10  12  12       2.0  \n",
       "...   ...       ...    ...   ...   ...     ...       ...  ..  ..  ..       ...  \n",
       "1435  ...         3      3     1     1       3         4   5   5   6       1.0  \n",
       "1436  ...         3      3     1     2       5         4   7   7   6       1.0  \n",
       "1437  ...         5      5     1     3       1        14   6   5   5       1.0  \n",
       "1438  ...         5      5     2     4       5        16   6   5   5       1.0  \n",
       "1439  ...         4      5     2     4       1        22   6   6   4       1.0  \n",
       "\n",
       "[1440 rows x 34 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encoding datasets and splitting datasets\n",
    "#DATASET ENCODING OF CATEGORICAL VARIABLES\n",
    "dataset_final_sampling1_copy = dataset_final_sampling1.copy() #copying the dataset to preserve original copy\n",
    "ord_enc_new = OrdinalEncoder() #encoder object\n",
    "\n",
    "#looping through all the categorical columns initailly found and applying encoding on it\n",
    "print('Performing EnCoding Of Categorical variables')\n",
    "\n",
    "dataset_final_sampling1_copy['G3_Class'] = ord_enc_new.fit_transform(dataset_final_sampling1_copy[['G3_Class']])\n",
    "\n",
    "features = dataset_final_sampling1_copy.drop(['G3','G3_Class'],axis=1)\n",
    "labels = dataset_final_sampling1_copy['G3_Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Support Vector Machine\n",
    "svc_model = SVC(C= .1, kernel='linear', gamma= 1)\n",
    "svc_model.fit(x_train, y_train)\n",
    "  \n",
    "prediction = svc_model.predict(x_test)\n",
    "# check the accuracy on the training set\n",
    "print(svc_model.score(x_train, y_train))\n",
    "print(svc_model.score(x_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9013e23377c4df86dc09c341e3e8e9dd81c02c4de203807e1a62a3a9bc38c81"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
